{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n",
      "Iteration: 20 / 200\n",
      "75.663%\n",
      "Iteration: 40 / 200\n",
      "84.598%\n",
      "Iteration: 60 / 200\n",
      "84.540%\n",
      "Iteration: 80 / 200\n",
      "88.477%\n",
      "Iteration: 100 / 200\n",
      "89.075%\n",
      "Iteration: 120 / 200\n",
      "88.908%\n",
      "Iteration: 140 / 200\n",
      "89.713%\n",
      "Iteration: 160 / 200\n",
      "90.392%\n",
      "Iteration: 180 / 200\n",
      "90.675%\n",
      "Iteration: 200 / 200\n",
      "90.873%\n"
     ]
    }
   ],
   "source": [
    "# This script trains the neural networks on the MNIST dataset of handwritten numbers.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def derivative_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    exp = np.exp(Z - np.max(Z))\n",
    "    return exp / exp.sum(axis=0)\n",
    "\n",
    "def init_params(size):\n",
    "    W1 = np.random.normal(size=(10, 784)) * np.sqrt(1./(784))\n",
    "    b1 = np.random.normal(size=(10, 1)) * np.sqrt(1./10)\n",
    "    W2 = np.random.normal(size=(10, 10)) * np.sqrt(1./20)\n",
    "    b2 = np.random.normal(size=(10, 1)) * np.sqrt(1./(784))\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "def forward_propagation(X,W1,b1,W2,b2):\n",
    "    Z1 = W1.dot(X) + b1 #10, m\n",
    "    A1 = ReLU(Z1) # 10,m\n",
    "    Z2 = W2.dot(A1) + b2 #10,m\n",
    "    A2 = softmax(Z2) #10,m\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y):\n",
    "    ''' return an 0 vector with 1 only in the position correspondind to the value in Y'''\n",
    "    one_hot_Y = np.zeros((Y.max()+1,Y.size))\n",
    "    one_hot_Y[Y,np.arange(Y.size)] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_propagation(X, Y, A1, A2, W2, Z1, m):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = 2*(A2 - one_hot_Y) #10,m\n",
    "    dW2 = 1/m * (dZ2.dot(A1.T)) # 10 , 10\n",
    "    db2 = 1/m * np.sum(dZ2,1) # 10, 1\n",
    "    dZ1 = W2.T.dot(dZ2)*derivative_ReLU(Z1) # 10, m\n",
    "    dW1 = 1/m * (dZ1.dot(X.T)) #10, 784\n",
    "    db1 = 1/m * np.sum(dZ1,1) # 10, 1\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(alpha, W1, b1, W2, b2, dW1, db1, dW2, db2):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * np.reshape(db1, (10,1))\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * np.reshape(db2, (10,1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y)/Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    size , m = X.shape\n",
    "\n",
    "    W1, b1, W2, b2 = init_params(size)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W2, Z1, m)\n",
    "\n",
    "        W1, b1, W2, b2 = update_params(alpha, W1, b1, W2, b2, dW1, db1, dW2, db2)\n",
    "\n",
    "        if (i+1) % int(iterations/10) == 0:\n",
    "            print(f\"Iteration: {i+1} / {iterations}\")\n",
    "            prediction = get_predictions(A2)\n",
    "            print(f'{get_accuracy(prediction, Y):.3%}')\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def make_predictions(X, W1 ,b1, W2, b2):\n",
    "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def show_prediction(index,X, Y, W1, b1, W2, b2):\n",
    "    vect_X = X[:, index,None]\n",
    "    prediction = make_predictions(vect_X, W1, b1, W2, b2)\n",
    "    label = Y[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "\n",
    "    current_image = vect_X.reshape((WIDTH, HEIGHT)) * SCALE_FACTOR\n",
    "\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "############## MAIN ##############\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255\n",
    "WIDTH = X_train.shape[1]\n",
    "HEIGHT = X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\n",
    "X_test = X_test.reshape(X_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR\n",
    "\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.15, 200)\n",
    "with open(\"trained_params_sample.pkl\",\"wb\") as dump_file:\n",
    "    pickle.dump((W1, b1, W2, b2),dump_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
